{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras import backend as K\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización de matrices de pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=2                           # for reproducibility \n",
    "np.random.seed(seed)\n",
    "\n",
    "w1 = np.random.normal(loc=0, scale=1, size=(3,)) #Capa 1 afectada por capa 0\n",
    "w2 = np.random.normal(loc=0, scale=1, size=(4,)) #Capa 2 afectada por capa 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patrones de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = 4 #Cantidad de patrones para entrenar\n",
    "x_train = np.zeros((ntrain, 3), dtype = np.float32) #Entradas\n",
    "y_train = np.zeros((ntrain, 1), dtype = np.float32) #Salidas\n",
    "\n",
    "x_train[0][0] = 1\n",
    "x_train[0][1] = 1\n",
    "y_train[0] = 1\n",
    "\n",
    "x_train[1][0] = 1\n",
    "x_train[1][1] = -1\n",
    "y_train[1] = -1\n",
    "\n",
    "x_train[2][0] = -1\n",
    "x_train[2][1] = 1\n",
    "y_train[2] = -1\n",
    "\n",
    "x_train[3][0] = -1\n",
    "x_train[3][1] = -1\n",
    "y_train[3] = 1\n",
    "\n",
    "#Neurona de bias en la capa de entrada\n",
    "for i in range(ntrain):\n",
    "    x_train[i][2] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_esp, y_obt):\n",
    "    '''\n",
    "    Recibe el vector de patrones de salidas esperadas y obtenidas (escalares) y calcula el error cuadrático\n",
    "    '''\n",
    "    e = 0\n",
    "    for i in range(len(y_esp)):\n",
    "        for j in range(len(y_esp[0])):\n",
    "            e += (y_esp[i][j] - y_obt[i][j])**2\n",
    "    \n",
    "    e = 0.5*e\n",
    "\n",
    "    return e\n",
    "\n",
    "def g(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def g_pr(x):\n",
    "    y = 1 - np.tanh(x)**2\n",
    "    return y\n",
    "\n",
    "def v1_accuracy(y_true, y_pred):\n",
    "    return np.mean(K.mean(K.equal(y_true, K.round(y_pred)), axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables para el loop de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 6 #Cantidad de realizaciones\n",
    "lrs = [0.01, 0.05, 0.1, 0.5, 1, 5] #learning rates\n",
    "epocas = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectores para luego analizar el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = np.zeros(shape=(epocas, r)) #Accuracy de la neurona\n",
    "mse = np.zeros(shape=(epocas, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(r):\n",
    "    lr = lrs[i]\n",
    "    #Módulos de las correcciones\n",
    "    deltas1 = np.zeros(epocas) #Módulo del vector deltaW1\n",
    "    deltas2 = np.zeros(epocas)  #Módulo del vector deltaW2\n",
    "\n",
    "    #Salidas de la red para cada época\n",
    "    salidas = np.zeros((ntrain, 1), dtype = np.float32) #Salidas #Registro las salidas predichas y calculo accuracy\n",
    "\n",
    "    for k in range(epocas):\n",
    "        deltaW1 = np.zeros(shape=(3,))\n",
    "        deltaW2 = np.zeros(shape=(4,))\n",
    "\n",
    "        for mu in range(ntrain):\n",
    "\n",
    "        #FORWARD PROPAGATION (entrada)\n",
    "            #Capa de entrada\n",
    "            c0 = x_train[mu]\n",
    "            # print(\"\\n\\nEntrada\")\n",
    "            # print(c0)\n",
    "\n",
    "            #Capa oculta\n",
    "            h1 = np.dot(c0,w1)\n",
    "            c1 = g(h1)\n",
    "            # print(\"\\nCapa oculta\")\n",
    "            # print(c0)\n",
    "            # print(w1)\n",
    "            # print(h1)\n",
    "            # print(c1)\n",
    "            \n",
    "\n",
    "            #Capa de salida\n",
    "            h2 = np.dot(np.concatenate([c0, np.array([c1])]), w2)\n",
    "            c2 = g(h2) #output\n",
    "            salidas[mu] = c2\n",
    "            # print(\"\\nSalida\")\n",
    "            # print(np.concatenate([c0, np.array([c1])]))\n",
    "            # print(w2)\n",
    "            # print(h2)\n",
    "            # print(c2)\n",
    "\n",
    "        #BACK PROPAGATION (delta)    \n",
    "            delta2 = (y_train[mu] - c2)*g_pr(h2)\n",
    "            # print(\"\\nBack propagation\")\n",
    "            # print(delta2)\n",
    "            # print(w2[3]) #No tengo que backpropagar el delta a la neurona del bias\n",
    "            # print(g_pr(h1))\n",
    "\n",
    "            delta1 = delta2*w2[3]*g_pr(h1)\n",
    "            # print(delta1)\n",
    "\n",
    "        #FORWARD PROPAGATION (correccion)\n",
    "            deltaW1 += lr * c0 * delta1 #doy vuelta c0 para obtener el producto vectorial\n",
    "            # print(\"\\nCorrecciones 1\")\n",
    "            # print(lr)\n",
    "            # print(c0)\n",
    "            # print(delta1)\n",
    "            # print(deltaW1)\n",
    "\n",
    "            deltaW2 += lr * np.concatenate([c0, [c1]]) * delta2\n",
    "            # print(\"\\nCorrecciones 2\")\n",
    "            # print(lr)\n",
    "            # print(np.concatenate([c0, [c1]]))\n",
    "            # print(delta2)\n",
    "            # print(deltaW2)\n",
    "\n",
    "\n",
    "        w1 += deltaW1\n",
    "        w2 += deltaW2\n",
    "\n",
    "        deltas1[k] = np.linalg.norm(deltaW1)\n",
    "        deltas2[k] = np.linalg.norm(deltaW2)\n",
    "        mse[k, i] = MSE(y_train, salidas)\n",
    "        accuracy[k, i] = v1_accuracy(y_train, salidas)\n",
    "\n",
    "\n",
    "    # print(\"Matrices de pesos finales\")\n",
    "    # print(w1)\n",
    "    # print(w2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lrs)):\n",
    "    plt.semilogy(range(1, epocas+1), mse[:,i], label = str(lrs[i]))\n",
    "\n",
    "plt.xlabel(\"Epoca\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guardo datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"D:\\IB/7mo cuatri\\Redes neuronales\\Practicas\\Practica4\")\n",
    "f_out = open(\"Ejercicio1/1b.txt\", \"w\")\n",
    "f_out.write(\"ep loss loss_stdv acc acc_stdv\\n\")\n",
    "for i in range(epocas):\n",
    "    f_out.write(str(i+1) + \" \" + str(mse_mean[i]) + \" \" + str(mse_std[i]) +  \" \" + str(acc_mean[i]) + \" \" + str(acc_std[i]) + \"\\n\")\n",
    "f_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
